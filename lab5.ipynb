{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import norm\n",
    "import random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Generate two 1D Gaussians (2 points)\n",
    "\n",
    "Here, you will generate **1D Gaussian (normal) distributions** using NumPy, and then visualize them with histograms and density curves.  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Generate two Gaussian-distributed datasets:\n",
    "   - The first centered at **0** with variance **1** (standard normal).\n",
    "   - The second centered at **5** with variance **1**.\n",
    "   - Use about **1000 samples** for each.\n",
    "\n",
    "This will help you see what it means for data to come from a *mixture of Gaussians*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 1D Gaussians\n",
    "# ----------------------------\n",
    "\n",
    "data1 = \n",
    "data2 = \n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"value\": np.concatenate([data1, data2]),\n",
    "    \"distribution\": [\"Gaussian 1\"] * len(data1) + [\"Gaussian 2\"] * len(data2)\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(\n",
    "    data=df, \n",
    "    x=\"value\", \n",
    "    hue=\"distribution\",\n",
    "    fill=True,       \n",
    "    common_norm=False,\n",
    "    alpha=0.5, \n",
    "    linewidth=2,\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "\n",
    "plt.title(\"Two 1D Gaussian Distributions\", fontsize=14)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Generate two 2D Gaussians (2 points)\n",
    "\n",
    "Now let’s move from **1D** to **2D** Gaussians.  \n",
    "A 2D Gaussian distribution is defined by:  \n",
    "- A **mean vector** (e.g., `[0, 0]`)  \n",
    "- A **covariance matrix** (e.g., `[[1, 0], [0, 1]]`)  \n",
    "\n",
    "**Your tasks:**\n",
    "1. Generate two 2D Gaussian-distributed datasets using `np.random.multivariate_normal`.\n",
    "   - First Gaussian: mean = `[0, 0]`, covariance = `[[1, 0.5], [0.5, 1]]`\n",
    "   - Second Gaussian: mean = `[3, 3]`, covariance = `[[1, -0.3], [-0.3, 1]]`\n",
    "   - Use about **500 samples** for each.\n",
    "\n",
    "This will help you understand how clusters look in **2D Gaussian space**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 2D Gaussians\n",
    "# ----------------------------\n",
    "\n",
    "mean1 = \n",
    "cov1 = \n",
    "data1 = \n",
    "\n",
    "mean2 = \n",
    "cov2 = \n",
    "data2 = \n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "# ----------------------------\n",
    "# 2D Density Surface\n",
    "# ----------------------------\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(data1[:, 0], data1[:, 1], alpha=0.5, color='blue', label=\"Gaussian 1\")\n",
    "plt.scatter(data2[:, 0], data2[:, 1], alpha=0.5, color='red', label=\"Gaussian 2\")\n",
    "\n",
    "sns.kdeplot(x=data1[:,0], y=data1[:,1], levels=10, color='blue', alpha=0.5)\n",
    "sns.kdeplot(x=data2[:,0], y=data2[:,1], levels=10, color='red', alpha=0.5)\n",
    "\n",
    "plt.title(\"Two 2D Gaussian Distributions (Contours)\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.legend()\n",
    "\n",
    "# ----------------------------\n",
    "# 3D Density Surface\n",
    "# ----------------------------\n",
    "\n",
    "x, y = np.meshgrid(np.linspace(-4, 8, 100), np.linspace(-4, 8, 100))\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "rv1 = multivariate_normal(mean1, cov1)\n",
    "rv2 = multivariate_normal(mean2, cov2)\n",
    "\n",
    "z1 = rv1.pdf(pos)\n",
    "z2 = rv2.pdf(pos)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "views = [(30, 45), (0, 120), (20, 200)]  \n",
    "titles = [\"View 1\", \"View 2\", \"View 3\"]\n",
    "\n",
    "for i, (elev, azim) in enumerate(views, 1):\n",
    "    ax = fig.add_subplot(1, 3, i, projection=\"3d\")\n",
    "    ax.plot_surface(x, y, z1, cmap=\"Blues\", linewidth=0, alpha=0.9)\n",
    "    ax.plot_surface(x, y, z2, cmap=\"Reds\", linewidth=0, alpha=0.9)\n",
    "\n",
    "    ax.set_title(titles[i-1])\n",
    "    ax.set_xlabel(\"X1\")\n",
    "    ax.set_ylabel(\"X2\")\n",
    "    ax.set_zlabel(\"Y\")\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Generate three 2D Gaussians (3 points)\n",
    "\n",
    "Now let’s go one step further and generate **three 2D Gaussian distributions**.  \n",
    "A 2D Gaussian distribution is defined by:  \n",
    "- A **mean vector** (e.g., `[0, 0]`)  \n",
    "- A **covariance matrix** (e.g., `[[1, 0], [0, 1]]`)  \n",
    "\n",
    "**Your tasks:**  \n",
    "1. Generate three 2D Gaussian-distributed datasets using `np.random.multivariate_normal`.  \n",
    "   - First Gaussian: mean = `[0, 0]`, covariance = `[[1, 0.3], [0.3, 1]]`  \n",
    "   - Second Gaussian: mean = `[4, 4]`, covariance = `[[1, -0.4], [-0.4, 1]]`  \n",
    "   - Third Gaussian: mean = `[0, 5]`, covariance = `[[1, 0.2], [0.2, 1]]`  \n",
    "   - Use about **500 samples** for each.  \n",
    "\n",
    "\n",
    "This will give you an intuition for how **multiple Gaussian clusters overlap** in 2D space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# TODO Generate 3 2D Gaussians\n",
    "# ----------------------------\n",
    "mean1 = \n",
    "cov1 = \n",
    "\n",
    "mean2 = \n",
    "cov2 = \n",
    "\n",
    "mean3 = \n",
    "cov3 = \n",
    "\n",
    "data1 = \n",
    "data2 = \n",
    "data3 = \n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "# --- Subplot 1: 2D scatter + KDE contours ---\n",
    "ax1 = fig.add_subplot(1, 4, 1)\n",
    "ax1.scatter(data1[:, 0], data1[:, 1], alpha=0.4, color=\"blue\", label=\"Gaussian 1\")\n",
    "ax1.scatter(data2[:, 0], data2[:, 1], alpha=0.4, color=\"red\", label=\"Gaussian 2\")\n",
    "ax1.scatter(data3[:, 0], data3[:, 1], alpha=0.4, color=\"green\", label=\"Gaussian 3\")\n",
    "\n",
    "sns.kdeplot(x=data1[:, 0], y=data1[:, 1], levels=8, color=\"blue\", alpha=0.6, ax=ax1)\n",
    "sns.kdeplot(x=data2[:, 0], y=data2[:, 1], levels=8, color=\"red\", alpha=0.6, ax=ax1)\n",
    "sns.kdeplot(x=data3[:, 0], y=data3[:, 1], levels=8, color=\"green\", alpha=0.6, ax=ax1)\n",
    "\n",
    "ax1.set_title(\"2D Gaussian Distributions\")\n",
    "ax1.set_xlabel(\"X1\")\n",
    "ax1.set_ylabel(\"X2\")\n",
    "ax1.legend()\n",
    "\n",
    "# --- Setup for 3D PDFs ---\n",
    "x, y = np.meshgrid(np.linspace(-4, 8, 100), np.linspace(-4, 8, 100))\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "rv1 = multivariate_normal(mean1, cov1)\n",
    "rv2 = multivariate_normal(mean2, cov2)\n",
    "rv3 = multivariate_normal(mean3, cov3)\n",
    "\n",
    "z1 = rv1.pdf(pos)\n",
    "z2 = rv2.pdf(pos)\n",
    "z3 = rv3.pdf(pos)\n",
    "\n",
    "# --- Subplots 2-4: Three 3D views ---\n",
    "views = [(30, 45), (0, 120), (20, 200)]\n",
    "titles = [\"3D View 1\", \"3D View 2\", \"3D View 3\"]\n",
    "\n",
    "for i, (view, title) in enumerate(zip(views, titles), start=2):\n",
    "    ax = fig.add_subplot(1, 4, i, projection=\"3d\")\n",
    "    ax.plot_surface(x, y, z1, cmap=\"Blues\", linewidth=0, alpha=0.8)\n",
    "    ax.plot_surface(x, y, z2, cmap=\"Reds\", linewidth=0, alpha=0.8)\n",
    "    ax.plot_surface(x, y, z3, cmap=\"Greens\", linewidth=0, alpha=0.8)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"X1\")\n",
    "    ax.set_ylabel(\"X2\")\n",
    "    ax.set_zlabel(\"Density\")\n",
    "    ax.view_init(elev=view[0], azim=view[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Train a Gaussian Mixture Model (GMM) using Expectation–Maximization (8 points)\n",
    "\n",
    "Now that you have generated three Gaussian distributions, let’s fit a **Gaussian Mixture Model (GMM)** to the data.  \n",
    "We will implement the **Expectation–Maximization (EM)** algorithm manually to show all steps:  \n",
    "\n",
    "1. **Initialization**  \n",
    "   - Randomly initialize the means, variances, and mixture weights (π).  \n",
    "\n",
    "2. **E-step**  \n",
    "   - Compute the \"responsibilities\", i.e., the probability that each point belongs to each Gaussian.  \n",
    "\n",
    "3. **M-step**  \n",
    "   - Update the means, covariances, and mixture weights using the responsibilities.  \n",
    "\n",
    "4. **Iterate** until convergence (or fixed number of steps).  \n",
    "\n",
    "This process maximizes the **likelihood** of the data under the mixture model.  \n",
    "\n",
    "Your tasks:  \n",
    "- Initialize Means, Covariances and weights.  \n",
    "- Implement the **E-step** and **M-step** functions.  \n",
    "- Train the model for ~20 iterations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1: Random Initialization (2 points)\n",
    "\n",
    "Before running the **Expectation–Maximization (EM)** algorithm, we need to **initialize the parameters** of our Gaussian Mixture Model (GMM).  \n",
    "Remember, GMM has three types of parameters:  \n",
    "\n",
    "- **Means (μ):** the center of each Gaussian cluster  \n",
    "- **Covariance matrices (Σ):** the spread/shape of each Gaussian  \n",
    "- **Weights (π):** the mixing proportions of each cluster  \n",
    "\n",
    "**Your task:**  \n",
    "1. Randomly pick **K points** from the dataset `X` as the initial means.  \n",
    "   - Hint: use `np.random.choice`.  \n",
    "2. Initialize each covariance matrix as the **overall covariance of the dataset**.  \n",
    "   - Hint: use `np.cov(X, rowvar=False)`.  \n",
    "3. Initialize the weights to be **equal** (e.g., `1/K` for each cluster).  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Stack all three distributions into one dataset\n",
    "X = np.vstack([data1, data2, data3])\n",
    "np.random.seed(42)\n",
    "\n",
    "# ----------------------------\n",
    "# TODO: Random Initialization \n",
    "# ----------------------------\n",
    "K =  \n",
    "\n",
    "means_init = \n",
    "covs_init = \n",
    "weights_init = \n",
    "\n",
    "# ----------------------------\n",
    "# Implementation Ends Here\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Initial Means:\\n\", means_init)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Expectation Step (E-step) (3 points)\n",
    "\n",
    "In the **E-step**, we compute the **responsibilities** — the probabilities that each data point belongs to each Gaussian cluster, given the current parameters.  \n",
    "\n",
    "Mathematically, for each point $x_i$ and cluster $k$:  \n",
    "\n",
    "$$\n",
    "r_{ik} = \\frac{\\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\pi_k$ = weight of cluster $k$  \n",
    "- $\\mu_k, \\Sigma_k$ = mean and covariance of cluster $k$  \n",
    "- $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ = Gaussian PDF  \n",
    "\n",
    "**Your task:**  \n",
    "1. For each cluster $k$, compute the Gaussian PDF values for all data points.  \n",
    "   - Hint: use `scipy.stats.multivariate_normal.pdf`.  \n",
    "2. Multiply each PDF by the corresponding cluster weight $\\pi_k$.  \n",
    "\n",
    "The output should be an **$N \\times K$ matrix** of responsibilities.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def e_step(X, means, covariances, weights):\n",
    "\n",
    "\n",
    "\n",
    "    N, D = X.shape\n",
    "    K = len(means)\n",
    "    resp = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        # ----------------------------\n",
    "        # TODO: Implement Expectation Step \n",
    "        # ----------------------------\n",
    "        rv = \n",
    "        resp[:, k] = \n",
    "\n",
    "        # ----------------------------\n",
    "        # Implementation Ends Here\n",
    "        # ----------------------------\n",
    "    resp /= resp.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Maximization Step (M-step) (3 points)\n",
    "\n",
    "In the **M-step**, we update the parameters of our Gaussian Mixture Model (means, covariances, and weights) using the responsibilities $r_{ik}$ from the E-step.  \n",
    "\n",
    "#### Parameter updates:\n",
    "\n",
    "- **Effective number of points assigned to cluster $k$:**\n",
    "$$\n",
    "N_k = \\sum_{i=1}^N r_{ik}\n",
    "$$\n",
    "\n",
    "- **Updated means:**\n",
    "$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} x_i\n",
    "$$\n",
    "\n",
    "- **Updated covariance matrices:**\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^N r_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "- **Updated weights:**\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "#### Your task:\n",
    "\n",
    "1. Use $N_k$ to update the **means**.  \n",
    "2. Update the **covariance matrices** by weighting squared deviations.  \n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def m_step(X, resp):\n",
    "\n",
    "    N, D = X.shape\n",
    "    K = resp.shape[1]\n",
    "    Nk = resp.sum(axis=0)  \n",
    "    means = np.dot(resp.T, X) / Nk[:, np.newaxis]\n",
    "    covariances = []\n",
    "    for k in range(K):\n",
    "        # ----------------------------\n",
    "        # TODO: Implement Maximization Step\n",
    "        # ----------------------------\n",
    "        \n",
    "        \n",
    "        # ----------------------------\n",
    "        # Implementation Ends Here\n",
    "        # ----------------------------\n",
    "    weights = Nk / N\n",
    "\n",
    "\n",
    "\n",
    "    return means, covariances, weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: Train GMM with Expectation-Maximization (4 points)\n",
    "\n",
    "Now that we have implemented the **E-step** and **M-step**, we can put everything together into a training loop.\n",
    "\n",
    "The EM algorithm iteratively refines the parameters until convergence:\n",
    "\n",
    "1. **E-step:**  \n",
    "   Compute the responsibilities $r_{ik}$ given the current parameters.\n",
    "\n",
    "2. **M-step:**  \n",
    "   Update the means, covariances, and weights using the formulas from Step 3.\n",
    "\n",
    "3. **Repeat:**  \n",
    "   Run E-step and M-step in a loop until convergence (or for a fixed number of iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Your task:\n",
    "- Complete the training loop for **n_iters** iterations:\n",
    "  - Call `e_step` to compute responsibilities.  \n",
    "  - Call `m_step` to update the parameters.  \n",
    "\n",
    "At the end, the function should return the **final means, covariances, and weights** of the trained GMM.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gmm(X, means, covariances, weights, n_iters=20):\n",
    "    \"\"\"\n",
    "    Train a Gaussian Mixture Model using EM.\n",
    "    \n",
    "    Parameters:\n",
    "        X : (n_samples, n_features) dataset\n",
    "        means, covariances, weights : initialized parameters\n",
    "        n_iters : number of iterations\n",
    "\n",
    "    Returns:\n",
    "        means, covariances, weights\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # TODO: Implement GMM Training (EM algorithm)\n",
    "    # ----------------------------\n",
    "\n",
    "   \n",
    "    # ----------------------------\n",
    "    # Implementation Ends Here\n",
    "    # ----------------------------\n",
    "    return means, covariances, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Visualization function\n",
    "# ----------------------------\n",
    "def visualize_training(X, K=3, n_iters=20):\n",
    "    np.random.seed(42)\n",
    "    means = X[np.random.choice(len(X), K, replace=False)]\n",
    "    covariances = [np.cov(X, rowvar=False)] * K\n",
    "    weights = np.ones(K) / K\n",
    "    \n",
    "    colors = [\"red\", \"blue\", \"green\"]  \n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(22, 16))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        resp = e_step(X, means, covariances, weights)\n",
    "        means, covariances, weights = m_step(X, resp)\n",
    "        \n",
    "        ax = axes[i]\n",
    "        ax.scatter(X[:,0], X[:,1], alpha=0.08, color=\"gray\")\n",
    "        \n",
    "\n",
    "        for k, color in enumerate(colors[:K]):\n",
    "            ax.scatter(means[k,0], means[k,1], c=color, marker=\"x\", s=70, linewidth=2)\n",
    "            \n",
    "            mu_str = np.round(means[k], 2)\n",
    "            cov_diag = np.round(np.diag(covariances[k]), 2)\n",
    "            ax.text(\n",
    "                0.02, 0.95 - 0.08*k,   \n",
    "                f\"μ{k+1}={mu_str.tolist()}, Σ diag={cov_diag.tolist()}\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=8, color=color, ha=\"left\", va=\"top\", family=\"monospace\"\n",
    "            )\n",
    "        \n",
    "        ax.set_title(f\"Iteration {i+1}\", fontsize=12, fontweight=\"bold\", pad=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run visualization\n",
    "# ----------------------------\n",
    "final_means, final_covs, final_weights = train_gmm(X, means_init, covs_init, weights_init, n_iters=20)\n",
    "visualize_training(X, K=3, n_iters=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Behavior of Gaussian Mixture Models (2 points)\n",
    "\n",
    "In this exercise, instead of implementing EM from scratch, we will use **Scikit-learn’s `GaussianMixture`** class.\n",
    "\n",
    "### How to use the GMM Library\n",
    "1. Import and create a model:\n",
    "   ```python\n",
    "   from sklearn.mixture import GaussianMixture\n",
    "   gmm = GaussianMixture(n_components=2, covariance_type=\"full\", random_state=42)\n",
    "\n",
    "### Means and Covariances Used\n",
    "\n",
    "We test GMM under different scenarios by **changing the means and covariances** of two Gaussians:\n",
    "\n",
    "- **Scenario 1:** Well-separated clusters with equal variances *(easy case)*.  \n",
    "- **Scenario 2:** Overlapping clusters where one has larger variance.  \n",
    "- **Scenario 3:** Both Gaussians have the same mean but very different variances.  \n",
    "\n",
    "These examples illustrate how GMM fits data under varying conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Limitations of GMM on 1D Gaussians\n",
    "# ----------------------------\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "scenarios = [\n",
    "    (0, 1, 5, 1),    # Case 1: equal variance, well separated\n",
    "    (0, 1, 2, 4),    # Case 2: overlap + different spread\n",
    "    (0, 0.5, 0, 5)   # Case 3: same mean, very different variances\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(scenarios), 2, figsize=(14, 4*len(scenarios)))\n",
    "\n",
    "for i, (m1, v1, m2, v2) in enumerate(scenarios):\n",
    "    # Generate data\n",
    "    data1 = np.random.normal(loc=m1, scale=np.sqrt(v1), size=1000)\n",
    "    data2 = np.random.normal(loc=m2, scale=np.sqrt(v2), size=1000)\n",
    "    X_ex = np.concatenate([data1, data2])[:, None]  \n",
    "    df_ex = pd.DataFrame({\n",
    "        \"value\": np.concatenate([data1, data2]),\n",
    "        \"distribution\": [\"Gaussian 1\"]*len(data1) + [\"Gaussian 2\"]*len(data2)\n",
    "    })\n",
    "    \n",
    "    # Plot ground truth PDFs\n",
    "    x_range = np.linspace(X_ex.min()-2, X_ex.max()+2, 1000)\n",
    "    pdf1 = norm.pdf(x_range, loc=m1, scale=np.sqrt(v1))\n",
    "    pdf2 = norm.pdf(x_range, loc=m2, scale=np.sqrt(v2))\n",
    "    \n",
    "    axes[i,0].plot(x_range, pdf1, 'b-', linewidth=2, label='Gaussian 1', alpha=0.8)\n",
    "    axes[i,0].plot(x_range, pdf2, 'r-', linewidth=2, label='Gaussian 2', alpha=0.8)\n",
    "    axes[i,0].fill_between(x_range, pdf1, alpha=0.3, color='blue')\n",
    "    axes[i,0].fill_between(x_range, pdf2, alpha=0.3, color='red')\n",
    "    \n",
    "    # Add data points as scatter plot\n",
    "    axes[i,0].scatter(data1, np.zeros_like(data1) + 0.1, alpha=0.3, s=1, color='blue')\n",
    "    axes[i,0].scatter(data2, np.zeros_like(data2) + 0.1, alpha=0.3, s=1, color='red')\n",
    "    \n",
    "    axes[i,0].set_title(f\"Scenario {i+1}: Ground Truth\")\n",
    "    axes[i,0].set_xlabel(\"X\")\n",
    "    axes[i,0].set_ylabel(\"Density\")\n",
    "    axes[i,0].legend()\n",
    "    \n",
    "    # ----------------------------\n",
    "    # TODO: SKlearn GMM training \n",
    "    # ----------------------------\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------\n",
    "    # Implementation ends here\n",
    "    # ----------------------------\n",
    "\n",
    "    # Plot GMM learned PDFs\n",
    "    # Get learned parameters\n",
    "    means_learned = gmm.means_.flatten()\n",
    "    covariances_learned = gmm.covariances_.flatten()\n",
    "    weights_learned = gmm.weights_\n",
    "    \n",
    "    # Plot individual component PDFs\n",
    "    pdf1_learned = norm.pdf(x_range, loc=means_learned[0], scale=np.sqrt(covariances_learned[0]))\n",
    "    pdf2_learned = norm.pdf(x_range, loc=means_learned[1], scale=np.sqrt(covariances_learned[1]))\n",
    "    \n",
    "    # Weighted mixture PDF\n",
    "    mixture_pdf = weights_learned[0] * pdf1_learned + weights_learned[1] * pdf2_learned\n",
    "    \n",
    "    axes[i,1].plot(x_range, pdf1_learned, 'b-', linewidth=2, label=f'Component 1 (μ={means_learned[0]:.2f})', alpha=0.8)\n",
    "    axes[i,1].plot(x_range, pdf2_learned, 'r-', linewidth=2, label=f'Component 2 (μ={means_learned[1]:.2f})', alpha=0.8)\n",
    "    axes[i,1].plot(x_range, mixture_pdf, 'k--', linewidth=2, label='Mixture PDF', alpha=0.8)\n",
    "    axes[i,1].fill_between(x_range, pdf1_learned, alpha=0.3, color='blue')\n",
    "    axes[i,1].fill_between(x_range, pdf2_learned, alpha=0.3, color='red')\n",
    "    \n",
    "    # Add data points colored by cluster assignment\n",
    "    cluster1_data = X_ex[labels == 0].flatten()\n",
    "    cluster2_data = X_ex[labels == 1].flatten()\n",
    "    \n",
    "    axes[i,1].scatter(cluster1_data, np.zeros_like(cluster1_data) + 0.1, alpha=0.3, s=1, color='blue')\n",
    "    axes[i,1].scatter(cluster2_data, np.zeros_like(cluster2_data) + 0.1, alpha=0.3, s=1, color='red')\n",
    "    \n",
    "    axes[i,1].set_title(f\"Scenario {i+1}: GMM Fit\")\n",
    "    axes[i,1].set_xlabel(\"X\")\n",
    "    axes[i,1].set_ylabel(\"Density\")\n",
    "    axes[i,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Questions (6 points)\n",
    "\n",
    "1. **Model Parameters:**  \n",
    "   - What are the **final means and covariance matrices** that the model converged to?  \n",
    "   - Do they align well with the Gaussian clusters you originally generated?  \n",
    "\n",
    "   **Answer:**  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **Longer Training (40 steps):**  \n",
    "   - What do you think would happen if we ran the EM algorithm for **40 steps** instead of 20?  \n",
    "\n",
    "   **Answer:**  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "3. **Early Stopping (10 steps):**  \n",
    "   - What would have happened if we had stopped after **10 steps**? Would the model have converged?  \n",
    "\n",
    "   **Answer:**  \n",
    "\n",
    "---\n",
    "4. **GMM with same means with different variance:**  \n",
    "   - In Scenario 3 of the Exercise 5, both the gaussians have same means but different variance. Why can GMM separate two components even though they share the same mean?\n",
    "\n",
    "   **Answer:**  \n",
    "\n",
    "   \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
